{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de Curso Procesamiento-Lenguaje-Natural.ipynb","provenance":[{"file_id":"11pkxwuHFjIPORdJ72nBUCEZE2Wb4Eazr","timestamp":1620062150601}],"mount_file_id":"11pkxwuHFjIPORdJ72nBUCEZE2Wb4Eazr","authorship_tag":"ABX9TyM34V58E7kKeY5evsJxjGMy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vk9faEemNxDm"},"source":["## **PRACTICA DE PROCESAMIENTO DE LENGUAJE NATURAL**"]},{"cell_type":"markdown","metadata":{"id":"68--SlTEIdFS"},"source":["**Pre-requisito**\n","\n","Descargaremos los textos de su Blog con cuentos de humor de los años 2004 a 2015\n"]},{"cell_type":"markdown","metadata":{"id":"YiCI29leIPK5"},"source":["**CONTENIDO**\n","\n","1 - Obtener datos\n","\n","2 - Cargar los datos\n","\n","3 - Limpiar datos\n","\n","4 - Analisis Exploratorio\n","\n","5 - Anáisis de Sentimiento\n","\n","6 - Modelado de Temáticas\n"]},{"cell_type":"code","metadata":{"id":"0ILSgheJItXb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jj-5r0uKIudE"},"source":["# Importamos las librerias"]},{"cell_type":"code","metadata":{"id":"3V1BQk8ZI1DO"},"source":["# imports\n","import requests\n","from bs4 import BeautifulSoup\n","import pickle\n","from time import sleep"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZxXUg7xmI345"},"source":["# 1 - Obtener los textos"]},{"cell_type":"code","metadata":{"id":"xeArUa8JI53F"},"source":["def url_to_transcript(url):\n","    '''Obtener los enlaces del blog de Hernan Casciari.'''\n","    page = requests.get(url).text\n","    soup = BeautifulSoup(page, \"lxml\")\n","    print('URL',url)\n","    enlaces = []\n","    for title in soup.find_all(class_=\"entry-title\"):\n","        for a in title.find_all('a', href=True):\n","            print(\"Found link:\", a['href'])\n","            enlaces.append(a['href'])\n","    sleep(0.75) #damos tiempo para que no nos penalice un firewall\n","    return enlaces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WLwTLc7BI-XE"},"source":["base = 'https://editorialorsai.com/category/epocas/'\n","urls = []\n","anios = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n","for anio in anios:\n","    urls.append(base + anio + \"/\")\n","urls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8KzkaYoJBde"},"source":["# Recorrer las URLs y obtener los enlaces\n","enlaces = [url_to_transcript(u) for u in urls]\n","print(enlaces)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rX6XYK8TJGHi"},"source":["def url_get_text(url):\n","    '''Obtener los textos de los cuentos de Hernan Casciari.'''\n","    print('URL',url)\n","    text=\"\"\n","    try:\n","        page = requests.get(url).text\n","        soup = BeautifulSoup(page, \"lxml\")\n","        text = [p.text for p in soup.find(class_=\"entry-content\").find_all('p')]\n","    except Exception:\n","        print('ERROR, puede que un firewall nos bloquea.')\n","        return ''\n","    sleep(0.75) #damos tiempo para que no nos penalice un firewall\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkjriYcdJIW8"},"source":["# Recorrer las URLs y obtener los textos\n","MAX_POR_ANIO = 50 # para no saturar el server\n","textos=[]\n","for i in range(len(anios)):\n","    arts = enlaces[i]\n","    arts = arts[0:MAX_POR_ANIO]\n","    textos.append([url_get_text(u) for u in arts])\n","print(len(textos))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5L6XEnBJK_r"},"source":["#Probamos a ver alguno de los textos\n","print(len(textos[0]))\n","print(textos[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4Tr-8FsJNJr"},"source":["# # Pickle files para usar luego\n","\n","# # Creamos un directorio y nombramos los archivos por año\n","!mkdir blog\n","\n","for i, c in enumerate(anios):\n","    with open(\"blog/\" + c + \".txt\", \"wb\") as file:\n","        cad=\"\"\n","        for texto in textos[i]:\n","            for texto0 in texto:\n","                cad=cad + texto0\n","        pickle.dump(cad, file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1Urrb20JPRb"},"source":["# 2 - Cargar los Datos"]},{"cell_type":"code","metadata":{"id":"F31xG7vCJQB7"},"source":["import pickle\n","\n","# Cargamos los pickled files\n","anios = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n","data = {}\n","for i, c in enumerate(anios):\n","    with open(\"blog/\" + c + \".txt\", \"rb\") as file:\n","        data[c] = pickle.load(file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQkfg0uOJUoo"},"source":["# Revisamos que se haya guardado bien\n","data.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"349GhfxoJXAB"},"source":["# Veamos algun trozo de texto\n","data['2008'][1000:1222]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmcOIMJ8JZFw"},"source":["# checkeamos primer clave\n","next(iter(data.keys()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqzrqqZvJbGy"},"source":["# nuestro diccionario esta cómo clave:Año valor:texto\n","next(iter(data.values()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVjNVrGrJdSI"},"source":["# lo combinamos\n","data_combined = {key: [value] for (key, value) in data.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ehTkDK0GJfVi"},"source":["# lo metemos en un Panda's dataframe\n","import pandas as pd\n","pd.set_option('max_colwidth',150)\n","\n","data_df = pd.DataFrame.from_dict(data_combined).transpose()\n","data_df.columns = ['transcript']\n","data_df = data_df.sort_index()\n","data_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ram1P-wUJh37"},"source":["# Veamos uno de los contenidos\n","data_df.transcript.loc['2007']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"theTix9AJjsi"},"source":["# 3 - Limpiar los Datos"]},{"cell_type":"code","metadata":{"id":"kFVJWLS3JmNk"},"source":["# Aplicaremos varios rounds de limpieza\n","import re\n","import string\n","\n","def clean_text_round1(text):\n","    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n","    text = text.lower()\n","    text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n","    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    return text\n","\n","round1 = lambda x: clean_text_round1(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k07GL-FkJoWR"},"source":["# vemos la primer limpieza\n","data_clean = pd.DataFrame(data_df.transcript.apply(round1))\n","data_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTY3nSwiJqkp"},"source":["# Segundo round\n","def clean_text_round2(text):\n","    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n","    text = re.sub('[‘’“”…«»]', '', text)\n","    text = re.sub('\\n', ' ', text)\n","    return text\n","\n","round2 = lambda x: clean_text_round2(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8NqQbrqJst5"},"source":["# veamos como queda\n","data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n","data_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ub0r0LTQJvB3"},"source":["# Let's take a look at our dataframe\n","#data_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xo79KhvJxLH"},"source":["# Como no tenemos un Lemmatizer en español, hacemos manualmente algunas conversiones\n","# OJO: esto realmente no se hace a mano!!!\n","\n","def detectadas(palabra):\n","    eliminar_s = ('libreros','textos','papelitos','monedas','páginas','anécdotas','perros','cuadernos','blogs',\n","                  'revistas','caballos','vecinos','madres','puntos','ricos','libros')\n","    if palabra in eliminar_s :\n","        return palabra[:-1]\n","    eliminar_es = ('mundiales','lectores','campeones','maníes','ustedes','autores')\n","    if palabra in eliminar_es:\n","        return palabra[:-2]\n","    return palabra\n","\n","def clean_text_round3(text):\n","    '''.'''\n","    return \" \".join([detectadas(word) for word in text.split()])\n","    \n","round3 = lambda x: clean_text_round3(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBgJKKBuJzne"},"source":["#vemos como queda\n","data_clean = pd.DataFrame(data_clean.transcript.apply(round3))\n","data_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_dhD_MvJ1kP"},"source":["# Esto es un nuevo campo por si quisieramos agregar alguna info adicional a cada año\n","# Nuestro caso repetimos los años, nos servirá para alguna visualización\n","full_names = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n","\n","data_df['full_name'] = full_names\n","data_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8aGMOGRuJ4Lp"},"source":["# Hacemos el pickle para usar más adelante\n","data_df.to_pickle(\"corpus.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wBkVgMeJ6IR"},"source":["data_clean.transcript[0:255]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oFOh5OLJ8uB"},"source":["# We are going to create a document-term matrix using CountVectorizer, and exclude common Spanish stop words\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","with open('spanish.txt') as f:\n","    lines = f.read().splitlines()\n","\n","cv = CountVectorizer(stop_words=lines)\n","data_cv = cv.fit_transform(data_clean.transcript)\n","data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n","data_dtm.index = data_clean.index\n","data_dtm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJo2h8teJ_Ke"},"source":["# Lo guardamos como pickle\n","data_dtm.to_pickle(\"dtm.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MH_osTn_KA-o"},"source":["# Lo guardamos como pickle también\n","data_clean.to_pickle('data_clean.pkl')\n","pickle.dump(cv, open(\"cv.pkl\", \"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9-R3J-UbKCsj"},"source":["# 4 - Análisis Exploratorio"]},{"cell_type":"code","metadata":{"id":"HE0kNbmaKFGv"},"source":["# Read in the document-term matrix\n","import pandas as pd\n","\n","data = pd.read_pickle('dtm.pkl')\n","data = data.transpose()\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ubz98_EgKJIF"},"source":["# Find the top 30 words (per Year)\n","top_dict = {}\n","for c in data.columns:\n","    top = data[c].sort_values(ascending=False).head(30)\n","    top_dict[c]= list(zip(top.index, top.values))\n","\n","top_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KEOJcepAKNVF"},"source":["# Print the top 15 words p/Year\n","for anio, top_words in top_dict.items():\n","    print(anio)\n","    print(', '.join([word for word, count in top_words[0:14]]))\n","    print('---')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9rBFmb7KKQgV"},"source":["# Look at the most common top words --> add them to the stop word list\n","from collections import Counter\n","\n","# Let's first pull out the top 30 words for each anio\n","words = []\n","for anio in data.columns:\n","    top = [word for (word, count) in top_dict[anio]]\n","    for t in top:\n","        words.append(t)\n","        \n","words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2pX4cMRKTY1"},"source":["# Let's aggregate this list and identify the most common words along with how many routines they occur in\n","Counter(words).most_common()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B57bFoQqKWgA"},"source":["# Las mas repetidas las descartaremos\n","add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n","add_stop_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RgpYSm1aKZJO"},"source":["import nltk\n","from nltk.corpus import PlaintextCorpusReader\n","corpus_root = '/Users/jbagnato/python_projects/blog' \n","wordlists = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n","#wordlists.fileids()\n","#pals = wordlists.words('2004.txt')\n","\n","cfd = nltk.ConditionalFreqDist(\n","        (word,genre)\n","        for genre in anios\n","        for w in wordlists.words(genre + '.txt')\n","        for word in ['casa','mundo','tiempo','vida']\n","        if w.lower().startswith(word) )\n","cfd.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rv2yIXAjKdtZ"},"source":["# Let's update our document-term matrix with the new list of stop words\n","from sklearn.feature_extraction import text \n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Read in cleaned data\n","data_clean = pd.read_pickle('data_clean.pkl')\n","\n","# Add new stop words\n","with open('spanish.txt') as f:\n","    stop_words = f.read().splitlines()\n","for pal in add_stop_words:\n","    stop_words.append(pal)\n","more_stop_words=['alex','andrés','asi','andres','así','año','alejandro','alfonso','allí','alguien',\n","                 'basdala','bernardo','bien',\n","                 'cosa','cosas','costoya','costa','cinco','celoni','cuatro','cómo','casi','colo','caprio','českomoravský','české','costa','canoso','carla','comequechu',\n","                 'dos','dice','decir','días','dije','digo','diez',\n","                 'ésa', 'ésas', 'ése', 'ésos', 'ésta', 'éstas', 'éste', 'ésto', 'éstos',\n","                 'fernando','fenwick',\n","                 'gelós','gente',\n","                 'hornby','hernan','hernán','hoy','horacio','horas','hará','hans','hacía','haber',\n","                 'iveta',\n","                 'jesús','jorge','juan',\n","                 'karen',\n","                 'lucas','luego', 'luis',\n","                 'mirta','mientras','menos','mónica','medio','mil','moncho','momento','mañana','mejor',\n","                 'narcís','número','noche','nadie',\n","                 'ojos',\n","                 'primer','primera','pase','pablo','pepe','pack','peter', 'pues','prieto','politto','pol','paola','puede','próximo','podrán','podía',\n","                 'quizá','quizás','quince','quién','quiero',\n","                 'rato',\n","                 'sólo','solamente','sakhan','šeredova','seis','šeredovà','seselovsky','solo','salas','sant','sino','se','sé','sabés','semana','soto','sido','solamente',\n","                 'tres','tan','todas','trece','toda','todavía','tarde','tener',\n","                 'uno','usted',\n","                 'veces','ver','ve','vos','va','voy',\n","                 'waiser','woung'\n","                ]\n","for pal in more_stop_words:\n","    stop_words.append(pal)\n","\n","# Recreate document-term matrix\n","cv = CountVectorizer(stop_words=stop_words)\n","data_cv = cv.fit_transform(data_clean.transcript)\n","data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n","data_stop.index = data_clean.index\n","\n","# Pickle it for later use\n","import pickle\n","pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n","data_stop.to_pickle(\"dtm_stop.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tyBsrxTIKgHq"},"source":["# Let's make some word clouds!\n","# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n","from wordcloud import WordCloud\n","\n","wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n","               max_font_size=150, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amkqg690KiMq"},"source":["# Reset the output dimensions\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams['figure.figsize'] = [16,12]\n","\n","anios = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n","\n","# Create subplots for each anio\n","for index, anio in enumerate(data.columns):\n","    wc.generate(data_clean.transcript[anio])\n","    plt.subplot(4, 3, index+1)\n","    plt.imshow(wc, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.title(anios[index])\n","    \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTItqzaCKlCy"},"source":["# Find the number of unique words per Year\n","\n","# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\n","unique_list = []\n","for anio in data.columns:\n","    uniques = data[anio].nonzero()[0].size\n","    unique_list.append(uniques)\n","\n","# Create a new dataframe that contains this unique word count\n","data_words = pd.DataFrame(list(zip(anios, unique_list)), columns=['Anio', 'unique_words'])\n","#data_unique_sort = data_words.sort_values(by='unique_words')\n","data_unique_sort = data_words # sin ordenar\n","data_unique_sort"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsCM2SBQKpap"},"source":["# ejecuta este si hicimos el webscrapping, o no tenemos los valores en la variable\n","posts_per_year=[]\n","try:\n","  enlaces\n","except NameError:\n","  # Si no hice, los tengo hardcodeados:\n","    posts_per_year = [50, 27, 18, 50, 42, 22, 50, 33, 31, 17, 33, 13]\n","else:\n","    for i in range(len(anios)):\n","        arts = enlaces[i]\n","        #arts = arts[0:10] #limito a maximo 10 por año\n","        print(anios[i],len(arts))\n","        posts_per_year.append(min(len(arts),MAX_POR_ANIO))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amgVpCfkKr8p"},"source":["# Calculate the words per post of each Year\n","\n","# Find the total number of words per Year\n","total_list = []\n","for anio in data.columns:\n","    totals = sum(data[anio])\n","    total_list.append(totals)\n","    \n","# Let's add some columns to our dataframe\n","data_words['total_words'] = total_list\n","data_words['posts_per_year'] = posts_per_year\n","data_words['words_per_posts'] = data_words['total_words'] / data_words['posts_per_year']\n","\n","# Sort the dataframe by words per minute to see who talks the slowest and fastest\n","#data_wpm_sort = data_words.sort_values(by='words_per_posts')\n","data_wpm_sort = data_words #sin ordenar\n","data_wpm_sort"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5DjmT8PKvQe"},"source":["# Let's plot our findings\n","import numpy as np\n","plt.rcParams['figure.figsize'] = [16, 6]\n","\n","y_pos = np.arange(len(data_words))\n","\n","plt.subplot(1, 3, 1)\n","plt.barh(y_pos,posts_per_year, align='center')\n","plt.yticks(y_pos, anios)\n","plt.title('Number of Posts', fontsize=20)\n","\n","\n","plt.subplot(1, 3, 2)\n","plt.barh(y_pos, data_unique_sort.unique_words, align='center')\n","plt.yticks(y_pos, data_unique_sort.Anio)\n","plt.title('Number of Unique Words', fontsize=20)\n","\n","plt.subplot(1, 3, 3)\n","plt.barh(y_pos, data_wpm_sort.words_per_posts, align='center')\n","plt.yticks(y_pos, data_wpm_sort.Anio)\n","plt.title('Number of Words Per Posts', fontsize=20)\n","\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_4MKc1-Kxme"},"source":["# 5 - Análisis de Sentimiento"]},{"cell_type":"code","metadata":{"id":"PvPWKM0dKySO"},"source":["# Leeremos el corpus que aún preserva el orden de las palabras\n","import pandas as pd\n","\n","data = pd.read_pickle('corpus.pkl')\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-uus0nZK1fr"},"source":["# Create quick lambda functions to find the polarity and subjectivity of each routine\n","# Terminal / Anaconda Navigator: conda install -c conda-forge textblob\n","from textblob import TextBlob\n","    \n","pol = lambda x: TextBlob(x).sentiment.polarity\n","pol2 = lambda x: x.sentiment.polarity\n","sub = lambda x: TextBlob(x).sentiment.subjectivity\n","sub2 = lambda x: x.sentiment.subjectivity\n","\n","# Realmente lo traducimos al inglés pues el analisis de sentimiento de TextBlob no funciona en Español :(\n","traducir = lambda x: TextBlob(x).translate(to=\"en\")\n","\n","data['blob_en'] = data['transcript'].apply(traducir)\n","data['polarity'] = data['blob_en'].apply(pol2)\n","data['subjectivity'] = data['blob_en'].apply(sub2)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvUb8UfoK4GC"},"source":["# Let's plot the results\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams['figure.figsize'] = [10, 8]\n","\n","for index, anio in enumerate(data.index):\n","    x = data.polarity.loc[anio]\n","    y = data.subjectivity.loc[anio]\n","    plt.scatter(x, y, color='blue')\n","    plt.text(x+.001, y+.001, data['full_name'][index], fontsize=10)\n","    plt.xlim(-0.051, 0.152) \n","    \n","plt.title('Sentiment Analysis', fontsize=20)\n","plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n","plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WIrpHF2EK67Y"},"source":["# Sentiment Over Time"]},{"cell_type":"code","metadata":{"id":"pbS6EVOAK9zQ"},"source":["# Split each routine into 12 parts\n","import numpy as np\n","import math\n","\n","def split_text(text, n=12):\n","    '''Takes in a string of text and splits into n equal parts, with a default of 12 equal parts.'''\n","\n","    # Calculate length of text, the size of each chunk of text and the starting points of each chunk of text\n","    length = len(text)\n","    size = math.floor(length / n)\n","    start = np.arange(0, length, size)\n","    \n","    # Pull out equally sized pieces of text and put it into a list\n","    split_list = []\n","    for piece in range(n):\n","        split_list.append(text[start[piece]:start[piece]+size])\n","    return split_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmkcMurBLBU3"},"source":["# Let's take a look at our data again\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMYClc2wLDRy"},"source":["# Let's create a list to hold all of the pieces of text\n","list_pieces = []\n","for t in data.blob_en:#transcript:\n","    split = split_text(t,12)\n","    list_pieces.append(split)   \n","#list_pieces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCurfcMvLFkN"},"source":["# The list has n elements, one for each transcript\n","len(list_pieces)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9HpNXwhLHpZ"},"source":["# Each transcript has been split into 10 pieces of text\n","len(list_pieces[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ya766BL2LJp2"},"source":["# Calculate the polarity for each piece of text\n","\n","polarity_transcript = []\n","for lp in list_pieces:\n","    polarity_piece = []\n","    for p in lp:\n","        #polarity_piece.append(TextBlob(p).translate(to=\"en\").sentiment.polarity)\n","        polarity_piece.append(p.sentiment.polarity)\n","    polarity_transcript.append(polarity_piece)\n","    \n","polarity_transcript"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TcFUbxBULNnn"},"source":["# Show the plot for one anio\n","plt.plot(polarity_transcript[0])\n","plt.title(data['full_name'].index[0])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9_CN657LPwW"},"source":["# Show the plot for all anios\n","plt.rcParams['figure.figsize'] = [16, 12]\n","\n","for index, anio in enumerate(data.index):    \n","    plt.subplot(3, 4, index+1)\n","    plt.plot(polarity_transcript[index])\n","    plt.plot(np.arange(0,12), np.zeros(12))\n","    plt.title(data['full_name'][index])\n","    plt.ylim(ymin=-.45, ymax=.45)\n","    \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IdqGWDEeLTsz"},"source":["# 6 - Modelado de Temáticas\n","\n","Realizaremos diversos intentos para obtener los temas que predominan en los cuentos"]},{"cell_type":"code","metadata":{"id":"-Bet-uKLLaCQ"},"source":["# Let's read in our document-term matrix\n","import pandas as pd\n","import pickle\n","\n","data = pd.read_pickle('dtm_stop.pkl')\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hj9AFyUuLhGt"},"source":["# Import the necessary modules for LDA with gensim\n","# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n","from gensim import matutils, models\n","import scipy.sparse\n","\n","# import logging\n","# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ErxS6ZIaLjQa"},"source":["# One of the required inputs is a term-document matrix\n","tdm = data.transpose()\n","tdm.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2fVtKQeLm0x"},"source":["# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n","sparse_counts = scipy.sparse.csr_matrix(tdm)\n","corpus = matutils.Sparse2Corpus(sparse_counts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FmEeGxUJLpiE"},"source":["# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n","cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n","id2word = dict((v, k) for k, v in cv.vocabulary_.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j12iQj54Lr5X"},"source":["# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n","# we need to specify two other parameters as well - the number of topics and the number of passes\n","lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n","lda.print_topics()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hk5EoTlJLuAf"},"source":["# LDA for num_topics = 3\n","lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n","lda.print_topics()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQ0KSGWLLwD1"},"source":["# LDA for num_topics = 4\n","lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n","lda.print_topics()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kvgEvCyPLzk-"},"source":["# Intento 2: sólo Sustantivos"]},{"cell_type":"code","metadata":{"id":"juXSdI9eL2d9"},"source":["# Let's create a function to pull out nouns from a string of text\n","from nltk import word_tokenize, pos_tag\n","\n","def nouns(text):\n","    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n","    is_noun = lambda pos: pos[:2] == 'NN'\n","    tokenized = word_tokenize(text,language='spanish')\n","    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n","    return ' '.join(all_nouns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8dfP5rnL40_"},"source":["data_clean = pd.read_pickle('data_clean.pkl')\n","data_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w99ZNbF3L7O3"},"source":["colname=[]\n","list_pieces = []\n","contador=0\n","for t in data_clean.transcript:\n","    split = split_text(t,posts_per_year[contador]-7)\n","    subcont=0\n","    for p in split:\n","        list_pieces.append(p)\n","        colname.append(str(2004+contador)+ \"-\" + str(subcont))\n","        subcont=subcont+1\n","    contador=contador+1\n","len(list_pieces)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWf4-fEwL9R6"},"source":["data_split = pd.DataFrame(data=list_pieces).transpose()\n","data_split.columns=colname\n","data_split2=data_split.transpose()\n","data_split2.columns = ['transcript']\n","data_split2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bk2SdXHoMBtI"},"source":["# Apply the nouns function to the transcripts to filter only on nouns\n","data_nouns = pd.DataFrame(data_split2.transcript.apply(nouns))\n","data_nouns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rz0tebJeMGMb"},"source":["# Create a new document-term matrix using only nouns\n","from sklearn.feature_extraction import text\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","with open('spanish.txt') as f:\n","    stop_words = f.read().splitlines()\n","for pal in add_stop_words:\n","    stop_words.append(pal)\n","for pal in more_stop_words:\n","    stop_words.append(pal)\n","\n","# Recreate a document-term matrix with only nouns\n","cvn = CountVectorizer(stop_words=stop_words)\n","data_cvn = cvn.fit_transform(data_nouns.transcript)\n","data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n","data_dtmn.index = data_nouns.index\n","data_dtmn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1YGckx6MK3M"},"source":["# Create the gensim corpus\n","corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n","\n","# Create the vocabulary dictionary\n","id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0FCr1DcMM7S"},"source":["# Let's start with 2 topics\n","ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n","ldan.print_topics()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8UYQLoELMPUC"},"source":["# Let's try topics = 3\n","ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n","ldan.print_topics()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"msvOJxdWMR74"},"source":["# Let's try topics = 4\n","ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n","ldan.print_topics()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9oe9sgeEMVVH"},"source":["# Intento 3: Sustantivos y adjetivos"]},{"cell_type":"code","metadata":{"id":"cYTYMIsGMX6B"},"source":["# Let's create a function to pull out nouns from a string of text\n","def nouns_adj(text):\n","    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n","    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n","    tokenized = word_tokenize(text,language='spanish')\n","    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n","    return ' '.join(nouns_adj)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zxo_5H_CMaJD"},"source":["# Apply the nouns function to the transcripts to filter only on nouns\n","data_nouns_adj = pd.DataFrame(data_split2.transcript.apply(nouns_adj)) #data_clean\n","data_nouns_adj"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5dZdES_oMddN"},"source":["# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n","cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n","data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n","data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n","data_dtmna.index = data_nouns_adj.index\n","data_dtmna"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-0nR4mUMhzl"},"source":["#data_dtmna['escritor']\n","print(data_dtmna.shape)\n","#print(cvna.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yop9ff9dMj9k"},"source":["# Create the gensim corpus\n","corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n","\n","# Create the vocabulary dictionary\n","id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwDBbZYyMmOp"},"source":["# Let's start with 2 topics\n","ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n","ldana.print_topics()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3X8WyCNMoSu"},"source":["# Let's start with 3 topics\n","ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n","ldana.print_topics()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"99cB-p8aMqlr"},"source":["# Probamos a modelar con 4 tópicos\n","ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n","ldana.print_topics()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ASnUO5b3Mtct"},"source":["# Identificar los temas"]},{"cell_type":"code","metadata":{"id":"z_xbgAeoMwSc"},"source":["# Our final LDA model\n","QTY_TOPICS=4\n","ldana = models.LdaModel(corpus=corpusna, num_topics=QTY_TOPICS, id2word=id2wordna, passes=40,\n","                        random_state=15)\n","ldana.print_topics(QTY_TOPICS,5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1H6YzoGMy0-"},"source":["import nltk\n","from nltk.corpus import PlaintextCorpusReader\n","corpus_root = '/Users/jbagnato/python_projects/blog' \n","wordlists = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n","#wordlists.fileids()\n","#pals = wordlists.words('2004.txt')\n","for i in range(QTY_TOPICS):\n","    theList=ldana.get_topic_terms(i)\n","\n","    cfd = nltk.ConditionalFreqDist(\n","        (word,genre)\n","        for genre in anios\n","        for w in wordlists.words(genre + '.txt')\n","        for word in [id2wordna.get(a) for (a,b) in theList]\n","        if w.lower().startswith(word) )\n","    cfd.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YO3Olt_7M2pe"},"source":["# Let's take a look at which topics each transcript contains\n","corpus_transformed = ldana[corpusna]\n","list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6OK3SX2M7O_"},"source":["Esto es lo que descubrimos:\n","\n","TEMA 0- Personas [2004,2009]\n","\n","TEMA 1- Medios de comunicación [2008,2010,2011,2012,2015]\n","\n","TEMA 2- Casciari [2005,2007]\n","\n","TEMA 3- Niñez / Infancia [2006,2013]\n"]},{"cell_type":"code","metadata":{"id":"RIC5zhPkM-iD"},"source":["#Info de Wikipedia\n","casciariTL = {2004:'blog gorda en españa. Nace su hija Nina.',\n","             2005:'premio alemania Deutsche Welle El mejor blog del mundo blog Más respeto, que soy tu madre ',\n","             2006:'Editorial Sudamericana publico en la Argentina y publica Diario de una mujer gorda',\n","             2007:'publicó su segundo libro, España deci alpiste. Colabora El PAis y La Nación',\n","             2008:'Gasalla se interesa por la obra teatro. ',\n","             2009:'se estrena en teatro. Le dió fama y mejora economica. Libro El pibe que arruinaba las fotos',\n","             2010:'renuncia a periódicos y funda Revista Orsai junto a Chiri, amigo de la infancia',\n","             2011:'Aparece primera edición de Orsai. Publica Charlas con mi hemisferio derecho',\n","             2012:'Inicia leyendo cuentos en radio Vorterix, por 2 años',\n","             2013:'Finaliza primera edicion Orsai',\n","             2014:'Edito revista tb para niños Bonsai',\n","             2015:'Publica El nuevo paraíso de los tontos. Se separa de su mujer. Sufre infarto y vuelve a la Argentina'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7A5N43nNEjd"},"source":["casciariTL"],"execution_count":null,"outputs":[]}]}